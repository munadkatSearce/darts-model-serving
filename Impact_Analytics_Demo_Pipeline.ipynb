{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaec34c-9ec7-4377-9f84-39297d850891",
   "metadata": {},
   "source": [
    "# Impact Analytics Sandbox Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0e984-e3eb-49ba-a30b-a0bd777dd628",
   "metadata": {},
   "source": [
    "The purpose of this demo is to demonstrate creation of kubeflow ML pipelines for Impact analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5338e4-6b34-448a-b317-7a3b290de33e",
   "metadata": {},
   "source": [
    "## Step 1 : Build and Push Base Container image\n",
    "We have created a base container which have all dependencies installed. Since, all dependencies are installed in the base container, we do not have to install the dependencies for every custom component repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6112db7-ed8c-45e4-afb6-80f0f1c57109",
   "metadata": {},
   "source": [
    "Build and Push container to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c4f9c-7119-42b8-a273-6942f84deab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI=\"gcr.io/impact-analytics-sandbox/base_container:v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9826fc-8000-44ad-aa59-33c7843952bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd base_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf3807-0bed-4510-a389-608e5f345452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build ./ -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f2c2d-925e-4aa7-9613-3f3f7800c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9135c6-e258-468c-b604-1c370dcc190f",
   "metadata": {},
   "source": [
    "## Step 2: Defining the pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb247cda-e00e-4b33-919e-3f961c2b71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google.cloud import aiplatform_v1\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d50ee3-39af-44c5-8eb7-337fe7ea3120",
   "metadata": {},
   "source": [
    "Since we are training a custom model using DARTS package we have created custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8bc1e-ed33-41ea-a0c9-174249ef4c06",
   "metadata": {},
   "source": [
    "### a) Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83810c2c-561f-4139-959f-093e87716dcd",
   "metadata": {},
   "source": [
    "Below component is used to fetch data from bigquery and its output will be passed as input to the training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4012151-4a48-4c19-b88e-ffd8ebf1a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom component to fetch data from BigQuery\n",
    "@component(\n",
    "    base_image=\"gcr.io/impact-analytics-sandbox/create_base_image:v1\",\n",
    "    output_component_file=\"create_dataset.yaml\"\n",
    ")\n",
    "def get_air_data(\n",
    "    #bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bqclient = bigquery.Client(project=\"impact-analytics-sandbox\")\n",
    "\n",
    "    # Download query results.\n",
    "    query_string = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `impact-analytics-sandbox.poc_dataset.AirPassengersDataset`\n",
    "    \"\"\"\n",
    "    # get dataframe by querying bigquery table\n",
    "    air_df = (\n",
    "        bqclient.query(query_string)\n",
    "            .result()\n",
    "            .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    air_df.to_csv(output_data_path,index=False)\n",
    "    print(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f08d4-f824-4e5c-a84f-780bebb51654",
   "metadata": {},
   "source": [
    "### b) Training and Evaluation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443db165-3134-4276-9500-9fc35b3811f8",
   "metadata": {},
   "source": [
    "Sequential_model component is used for model training and evaluation. We are transforming data, creating and saving the darts model based on model type parameter and evaluate model based on MAPE, MSE, and RMSE metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e495500-bb09-4416-bb48-4f7ba386d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom component for Model training and evaluation\n",
    "@component(\n",
    "    base_image=\"gcr.io/impact-analytics-sandbox/create_base_image:v1\",\n",
    "    output_component_file=\"beans_model_component.yaml\",\n",
    ")\n",
    "def sequential_model(\n",
    "    dataset:  Input[Dataset],\n",
    "    model_type: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    model_path: OutputPath(\"Model\"),\n",
    ") -> NamedTuple('ExampleOutputs', [('tar_path', str)]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    from darts import TimeSeries\n",
    "    from darts.utils.timeseries_generation import (\n",
    "        gaussian_timeseries,\n",
    "        linear_timeseries,\n",
    "        sine_timeseries,\n",
    "    )\n",
    "    from darts.models import RNNModel\n",
    "    from darts.metrics import mape, mse, rmse\n",
    "    from darts.dataprocessing.transformers import Scaler\n",
    "    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "    from google.cloud import storage\n",
    "    import glob\n",
    "    import shutil\n",
    "    from collections import namedtuple\n",
    "    from typing import NamedTuple\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"File {} uploaded to {}.\".format(\n",
    "                source_file_name, destination_blob_name\n",
    "            )\n",
    "        )    \n",
    "    # Data Preparation\n",
    "    air_df = pd.read_csv(dataset.path)\n",
    "    air_df['Month']=pd.to_datetime(air_df['Month'])\n",
    "    air_df.sort_values(by=\"Month\",inplace=True)\n",
    "    air_df.index = air_df['Month']\n",
    "    air_df.drop(\"Month\",inplace=True,axis=1)\n",
    "    series = TimeSeries.from_dataframe(air_df)\n",
    "    # Create training and validation sets:\n",
    "    train, val = series.split_after(pd.Timestamp(\"19590101\"))\n",
    "\n",
    "    # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "    transformer = Scaler()\n",
    "    train_transformed = transformer.fit_transform(train)\n",
    "    val_transformed = transformer.transform(val)\n",
    "    series_transformed = transformer.transform(series)\n",
    "\n",
    "    # create month and year covariate series\n",
    "    year_series = datetime_attribute_timeseries(\n",
    "        pd.date_range(start=series.start_time(), freq=series.freq_str, periods=1000),\n",
    "        attribute=\"year\",\n",
    "        one_hot=False,\n",
    "    )\n",
    "    year_series = Scaler().fit_transform(year_series)\n",
    "    month_series = datetime_attribute_timeseries(\n",
    "        year_series, attribute=\"month\", one_hot=True\n",
    "    )\n",
    "    covariates = year_series.stack(month_series)\n",
    "    cov_train, cov_val = covariates.split_after(pd.Timestamp(\"19590101\"))\n",
    "    \n",
    "    \n",
    "    #setting hyperparameters\n",
    "    hidden_dim=20\n",
    "    dropout=0\n",
    "    batch_size=16\n",
    "    epochs=300\n",
    "    learning_rate=1e-3\n",
    "    optimizer_kwargs={\"lr\":learning_rate }\n",
    "    model_name=\"Air_\"+model_type\n",
    "    log_tensorboard=True\n",
    "    random_state=42\n",
    "    training_length=20\n",
    "    input_chunk_length=14\n",
    "    force_reset=True\n",
    "    save_checkpoints=True\n",
    "    \n",
    "    # Model Creation\n",
    "    my_model = RNNModel(\n",
    "        model=model_type,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=epochs,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        model_name=model_name,\n",
    "        log_tensorboard=log_tensorboard,\n",
    "        random_state=random_state,\n",
    "        training_length=training_length,\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        force_reset=True,\n",
    "        save_checkpoints=True,\n",
    "    )\n",
    "    \n",
    "    my_model.fit(\n",
    "        train_transformed,\n",
    "        future_covariates=covariates,\n",
    "        val_series=val_transformed,\n",
    "        val_future_covariates=covariates,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # metadata about model\n",
    "    model.metadata[\"hidden_dim\"] = hidden_dim\n",
    "    model.metadata[\"dropout\"] = dropout\n",
    "    model.metadata[\"batch_size\"] = batch_size\n",
    "    model.metadata[\"n_epochs\"]=  epochs\n",
    "    model.metadata[\"learning rate\"] = learning_rate\n",
    "    #model.metadata[\"model_name\"]=\"Air_\"+model_type\n",
    "    \n",
    "    model.metadata[\"random_state\"] = random_state\n",
    "    model.metadata[\"training_length\"] = training_length\n",
    "    model.metadata[\"input_chunk_length\"] = input_chunk_length\n",
    "     \n",
    "    from datetime import datetime\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    k = glob.glob('darts_logs' + \"/**\", recursive=True)\n",
    "    blobs_list = []\n",
    "    for i in k:\n",
    "        if '.' in i:\n",
    "            blobs_list.append(i)\n",
    "\n",
    "    bucket_name = 'impact-analytics-experiments-bucket01'\n",
    "    \n",
    "    # Saving ML models to bucket\n",
    "    for blob in blobs_list:\n",
    "        print(blob)    \n",
    "        source_file_name = blob \n",
    "        destination_blob_name = 'model_logs{}/'.format(TIMESTAMP)+blob\n",
    "        upload_blob(bucket_name, source_file_name, destination_blob_name)\n",
    "        if blob.endswith(\".pth.tar\"):\n",
    "            path = bucket_name + \"/\" + destination_blob_name\n",
    "\n",
    "    my_model.save_model(model_path+ \".pth.tar\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Evaluating model\n",
    "    def eval_model(model):\n",
    "        pred_series = model.predict(n=26, future_covariates=covariates)\n",
    "        mape1 = mape(pred_series, val_transformed)\n",
    "        mse1 = mse(pred_series, val_transformed)\n",
    "        rmse1 = rmse(pred_series, val_transformed)\n",
    "        print(\"MAPE: {:.2f}%\".format(mape1))\n",
    "        print(\"MSE: {:.2f}%\".format(mse1))\n",
    "        print(\"RMSE: {:.2f}%\".format(rmse1))\n",
    "        metrics.log_metric(\"MAPE\",\"{:.2f}%\".format(mape1))\n",
    "        metrics.log_metric(\"MSE\", \"{}\".format(mse1))\n",
    "        metrics.log_metric(\"RMSE\", \"{}\".format(rmse1))\n",
    "    \n",
    "    eval_model(my_model)\n",
    "    example_output = namedtuple('ExampleOutputs', ['tar_path'])\n",
    "    return example_output(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76562456-b7f4-4856-8242-289c465b7d62",
   "metadata": {},
   "source": [
    "### c) Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924788ca-d40b-4f19-9824-1725fa42ef37",
   "metadata": {},
   "source": [
    "Deploy model component is for deploying model. However, we are getting a permission error on deploying the model in the pipeline. But we are able to deploy the model manually and the model deployment can be worked upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e3ecce-ae3a-45a8-8545-d709a1df7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/impact-analytics-sandbox/base_container:v3\",\n",
    "    output_component_file=\"deployment_component.yaml\",\n",
    "    )\n",
    "def deploy_to_run(\n",
    "    tar_path: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "    ):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from github import Github\n",
    "\n",
    "   \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(tar_path.split(\"/\")[0])\n",
    "    blob = bucket.blob(\"/\".join(tar_path.split(\"/\")[1:]))\n",
    "    blob.download_to_filename(\"/tmp/model.pth.tar\")\n",
    "\n",
    "    g = Github(\"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\")\n",
    "\n",
    "    repo = g.get_repo(\"munadkatSearce/darts-model-serving\")\n",
    "    contents = repo.get_contents(\"model.pth.tar\", ref=\"main\")\n",
    "\n",
    "    model_file = open(\"/tmp/model.pth.tar\", \"rb\")\n",
    "    file_content=model_file.read()\n",
    "    model_file.close()\n",
    "\n",
    "    repo.update_file(path=contents.path, message=\"new_model_trained\", content=file_content, sha=contents.sha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a28ad61-e664-4d4c-86fb-60273a2e1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'air-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea9e28-c42d-451b-aec6-22167bc6aa94",
   "metadata": {},
   "source": [
    "We are setting the global variables to pass in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cc693-a2a0-429a-94f7-46d4858cf7c3",
   "metadata": {},
   "source": [
    "## Step 3: Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff1cd10-c9f0-4e8a-8fd9-d833dbe28cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://impact-analytics-sandbox-bucket/pipeline_root_air/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Global variables\n",
    "PROJECT_ID=\"impact-analytics-sandbox\"\n",
    "REGION = \"us-central1\"\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-bucket\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_air/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f8529-150d-4681-b8c9-1776ac5e883f",
   "metadata": {},
   "source": [
    "Below is the code to define ML pipeline. The pipeline first fetches data from bigquery source, it then trains 3 models sequentially and deploys them to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02445af6-2781-4a67-812c-4a3397fd3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pipeline\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"sequential-pipeline\",\n",
    "    \n",
    ")\n",
    "def pipeline(\n",
    "    #url: str = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    #thresholds_dict_str: str = '{\"roc\":0.8}',\n",
    "    serving_container_image_uri: str = \"gcr.io/impact-analytics-sandbox/base_container:v3\"\n",
    "    ):\n",
    "    \n",
    "    # fetch data\n",
    "    data_op = get_air_data()\n",
    "    \n",
    "    # train and evaluate mulitple models\n",
    "    train_model_op_lstm = sequential_model(model_type=\"LSTM\",dataset= data_op.output)\n",
    "    train_model_op_GRU = sequential_model(model_type=\"GRU\", dataset=data_op.output).after(train_model_op_lstm)\n",
    "    train_model_op_RNN = sequential_model(model_type=\"RNN\", dataset=data_op.output).after(train_model_op_GRU)\n",
    "    \n",
    "    # deploy models\n",
    "    deploy_task1 = deploy_to_run(\n",
    "        train_model_op_lstm.outputs[\"tar_path\"],\n",
    "    )\n",
    "    deploy_task2 = deploy_to_run(\n",
    "        train_model_op_GRU.outputs[\"tar_path\"],\n",
    "    )\n",
    "    deploy_task3 = deploy_to_run(\n",
    "        train_model_op_RNN.outputs[\"tar_path\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7ddf7-a19b-4073-9b61-5ed3dd6ed2f7",
   "metadata": {},
   "source": [
    "Pipeline compiler will compile the pipeline and store the pipeline configuration inside a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a636f01-7f2b-4e5a-a54c-625adb3dadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"custom_train_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e60cc2fc-6741-41b7-8994-7a8d686bbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81122313-6fa7-4a07-a30e-e970e707fffd",
   "metadata": {},
   "source": [
    "## Step 4: Running the Pipeline Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee7356-bf67-4d10-8c86-aa134daab98d",
   "metadata": {},
   "source": [
    "Here we are deploying pipeline job which will be submitted for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "513a5d51-2bdb-4c12-93c7-a786a8be67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"custom-train-pipeline\",\n",
    "    template_path=\"custom_train_pipeline.json\",\n",
    "    job_id=\"custom-train-pipeline-{0}\".format(TIMESTAMP),\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a3308-1153-45ce-9d4c-06200b58a4c8",
   "metadata": {},
   "source": [
    "Below code will submit job to create the pipeline, you can use the link at the bottom to view pipeline status. Below link will help us monitor the execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc4a2ddb-7a8d-4a97-b8a1-ecfb4aa3b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/90786640424/locations/us-central1/pipelineJobs/custom-train-pipeline-20220301111833\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/90786640424/locations/us-central1/pipelineJobs/custom-train-pipeline-20220301111833')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-train-pipeline-20220301111833?project=90786640424\n"
     ]
    }
   ],
   "source": [
    "# Submit pipeline job\n",
    "pipeline_job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
